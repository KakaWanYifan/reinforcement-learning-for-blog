{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n",
      "动作空间 = Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import gym\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "from tensorflow import keras\n",
    "\n",
    "class AdvantageActorCriticAgent():\n",
    "    def __init__(self, env, actor_kwargs, critic_kwargs, gamma=0.99):\n",
    "        self.action_n = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.discount = 1.\n",
    "\n",
    "        self.actor_net = self.build_network(output_size=self.action_n,output_activation=tf.nn.softmax,loss=tf.losses.categorical_crossentropy,**actor_kwargs)\n",
    "        self.critic_net = self.build_network(output_size=1,**critic_kwargs)\n",
    "\n",
    "    def build_network(self, hidden_sizes, output_size, input_size=None,activation=tf.nn.relu, output_activation=None,loss=tf.losses.mse, learning_rate=0.01):\n",
    "        model = keras.Sequential()\n",
    "        for idx, hidden_size in enumerate(hidden_sizes):\n",
    "            kwargs = {}\n",
    "            if idx == 0 and input_size is not None:\n",
    "                kwargs['input_shape'] = (input_size,)\n",
    "            model.add(keras.layers.Dense(units=hidden_size,activation=activation, **kwargs))\n",
    "        model.add(keras.layers.Dense(units=output_size,activation=output_activation))\n",
    "        optimizer = tf.optimizers.Adam(learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        return model\n",
    "        \n",
    "    def decide(self, observation):\n",
    "        probs = self.actor_net.predict(observation[np.newaxis])[0]\n",
    "        action = np.random.choice(self.action_n, p=probs)\n",
    "        return action\n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation, done):\n",
    "        x = observation[np.newaxis]\n",
    "        u = reward + (1. - done) * self.gamma * self.critic_net.predict(next_observation[np.newaxis])\n",
    "        td_error = u - self.critic_net.predict(x)\n",
    "\n",
    "        # 训练执行者网络\n",
    "        x_tensor = tf.convert_to_tensor(observation[np.newaxis],dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pi_tensor = self.actor_net(x_tensor)[0, action]\n",
    "            logpi_tensor = tf.math.log(tf.clip_by_value(pi_tensor,1e-6, 1.))\n",
    "            loss_tensor = -self.discount * td_error * logpi_tensor\n",
    "        grad_tensors = tape.gradient(loss_tensor, self.actor_net.variables)\n",
    "        self.actor_net.optimizer.apply_gradients(zip(grad_tensors, self.actor_net.variables)) # 更新执行者网络\n",
    "\n",
    "        # 训练评论者网络\n",
    "        self.critic_net.fit(x, u, verbose=0) # 更新评论者网络\n",
    "\n",
    "        if done:\n",
    "            self.discount = 1. # 为下一回合初始化累积折扣\n",
    "        else:\n",
    "            self.discount *= self.gamma # 进一步累积折扣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_qlearning(env, agent, train=False, render=False):\n",
    "    print('play_qlearning')\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation, action, reward, next_observation,done)\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        observation = next_observation\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_qlearning\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea754551f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea754551f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea754ed040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea754ed040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fea75499820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fea75499820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "0  -419.0\n",
      "play_qlearning\n",
      "1  -500.0\n",
      "play_qlearning\n",
      "2  -500.0\n",
      "play_qlearning\n",
      "3  -500.0\n",
      "play_qlearning\n",
      "4  -500.0\n",
      "play_qlearning\n",
      "5  -500.0\n",
      "play_qlearning\n",
      "6  -500.0\n",
      "play_qlearning\n",
      "7  -500.0\n",
      "play_qlearning\n",
      "8  -500.0\n",
      "play_qlearning\n",
      "9  -500.0\n",
      "play_qlearning\n",
      "10  -227.0\n",
      "play_qlearning\n",
      "11  -401.0\n",
      "play_qlearning\n",
      "12  -392.0\n",
      "play_qlearning\n",
      "13  -175.0\n",
      "play_qlearning\n",
      "14  -118.0\n",
      "play_qlearning\n",
      "15  -166.0\n",
      "play_qlearning\n",
      "16  -158.0\n",
      "play_qlearning\n",
      "17  -155.0\n",
      "play_qlearning\n",
      "18  -227.0\n",
      "play_qlearning\n",
      "19  -141.0\n",
      "play_qlearning\n",
      "20  -131.0\n",
      "play_qlearning\n",
      "21  -112.0\n",
      "play_qlearning\n",
      "22  -151.0\n",
      "play_qlearning\n",
      "23  -149.0\n",
      "play_qlearning\n",
      "24  -118.0\n",
      "play_qlearning\n",
      "25  -170.0\n",
      "play_qlearning\n",
      "26  -114.0\n",
      "play_qlearning\n",
      "27  -130.0\n",
      "play_qlearning\n",
      "28  -130.0\n",
      "play_qlearning\n",
      "29  -109.0\n",
      "play_qlearning\n",
      "30  -228.0\n",
      "play_qlearning\n",
      "31  -145.0\n",
      "play_qlearning\n",
      "32  -128.0\n",
      "play_qlearning\n",
      "33  -115.0\n",
      "play_qlearning\n",
      "34  -150.0\n",
      "play_qlearning\n",
      "35  -105.0\n",
      "play_qlearning\n",
      "36  -119.0\n",
      "play_qlearning\n",
      "37  -165.0\n",
      "play_qlearning\n",
      "38  -116.0\n",
      "play_qlearning\n",
      "39  -94.0\n",
      "play_qlearning\n",
      "40  -118.0\n",
      "play_qlearning\n",
      "41  -104.0\n",
      "play_qlearning\n",
      "42  -189.0\n",
      "play_qlearning\n",
      "43  -124.0\n",
      "play_qlearning\n",
      "44  -98.0\n",
      "play_qlearning\n",
      "45  -105.0\n",
      "play_qlearning\n",
      "46  -92.0\n",
      "play_qlearning\n",
      "47  -175.0\n",
      "play_qlearning\n",
      "48  -101.0\n",
      "play_qlearning\n",
      "49  -108.0\n",
      "play_qlearning\n",
      "50  -131.0\n",
      "play_qlearning\n",
      "51  -98.0\n",
      "play_qlearning\n",
      "52  -120.0\n",
      "play_qlearning\n",
      "53  -110.0\n",
      "play_qlearning\n",
      "54  -125.0\n",
      "play_qlearning\n",
      "55  -123.0\n",
      "play_qlearning\n",
      "56  -80.0\n",
      "play_qlearning\n",
      "57  -107.0\n",
      "play_qlearning\n",
      "58  -108.0\n",
      "play_qlearning\n",
      "59  -128.0\n",
      "play_qlearning\n",
      "60  -138.0\n",
      "play_qlearning\n",
      "61  -114.0\n",
      "play_qlearning\n",
      "62  -90.0\n",
      "play_qlearning\n",
      "63  -115.0\n",
      "play_qlearning\n",
      "64  -98.0\n",
      "play_qlearning\n",
      "65  -86.0\n",
      "play_qlearning\n",
      "66  -99.0\n",
      "play_qlearning\n",
      "67  -95.0\n",
      "play_qlearning\n",
      "68  -109.0\n",
      "play_qlearning\n",
      "69  -84.0\n",
      "play_qlearning\n",
      "70  -104.0\n",
      "play_qlearning\n",
      "71  -70.0\n",
      "play_qlearning\n",
      "72  -131.0\n",
      "play_qlearning\n",
      "73  -95.0\n",
      "play_qlearning\n",
      "74  -82.0\n",
      "play_qlearning\n",
      "75  -83.0\n",
      "play_qlearning\n",
      "76  -121.0\n",
      "play_qlearning\n",
      "77  -103.0\n",
      "play_qlearning\n",
      "78  -102.0\n",
      "play_qlearning\n",
      "79  -183.0\n",
      "play_qlearning\n",
      "80  -140.0\n",
      "play_qlearning\n",
      "81  -126.0\n",
      "play_qlearning\n",
      "82  -76.0\n",
      "play_qlearning\n",
      "83  -175.0\n",
      "play_qlearning\n",
      "84  -68.0\n",
      "play_qlearning\n",
      "85  -69.0\n",
      "play_qlearning\n",
      "86  -77.0\n",
      "play_qlearning\n",
      "87  -73.0\n",
      "play_qlearning\n",
      "88  -106.0\n",
      "play_qlearning\n",
      "89  -109.0\n",
      "play_qlearning\n",
      "90  -99.0\n",
      "play_qlearning\n",
      "91  -70.0\n",
      "play_qlearning\n",
      "92  -77.0\n",
      "play_qlearning\n",
      "93  -102.0\n",
      "play_qlearning\n",
      "94  -99.0\n",
      "play_qlearning\n",
      "95  -73.0\n",
      "play_qlearning\n",
      "96  -81.0\n",
      "play_qlearning\n",
      "97  -76.0\n",
      "play_qlearning\n",
      "98  -107.0\n",
      "play_qlearning\n",
      "99  -63.0\n"
     ]
    }
   ],
   "source": [
    "actor_kwargs = {'hidden_sizes' : [100,], 'learning_rate' : 0.0001}\n",
    "critic_kwargs = {'hidden_sizes' : [100,], 'learning_rate' : 0.0002}\n",
    "agent = AdvantageActorCriticAgent(env, actor_kwargs=actor_kwargs,critic_kwargs=critic_kwargs)\n",
    "\n",
    "# 训练\n",
    "episodes = 100\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_qlearning(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('{}  {}'.format(episode,episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "平均回合奖励 = -9390.0 / 100 = -93.9\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "episode_rewards = [play_qlearning(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),len(episode_rewards), np.mean(episode_rewards)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
