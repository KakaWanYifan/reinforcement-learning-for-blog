{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Box(-8.0, 8.0, (3,), float32)\n",
      "动作空间 = Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(0)\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 经历回放\n",
    "class DQNReplayer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = pd.DataFrame(index=range(capacity),columns=['observation', 'action', 'reward','next_observation', 'done'])\n",
    "        self.i = 0\n",
    "        self.count = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def store(self, *args):\n",
    "        self.memory.loc[self.i] = args\n",
    "        self.i = (self.i + 1) % self.capacity\n",
    "        self.count = min(self.count + 1, self.capacity)\n",
    "\n",
    "    def sample(self, size):\n",
    "        indices = np.random.choice(self.count, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in self.memory.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, size, mu=0., sigma=1., theta=.15, dt=.01):\n",
    "        self.size = size\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "\n",
    "    def __call__(self):\n",
    "        n = np.random.normal(size=self.size)\n",
    "        self.x += (self.theta * (self.mu - self.x) * self.dt +self.sigma * np.sqrt(self.dt) * n)\n",
    "        return self.x\n",
    "\n",
    "    def reset(self, x=0.):\n",
    "        self.x = x * np.ones(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "from tensorflow import keras\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, env, actor_kwargs, critic_kwargs,replayer_capacity=20000, replayer_initial_transitions=2000,gamma=0.99, batches=1, batch_size=64,net_learning_rate=0.005, noise_scale=0.1, explore=True):\n",
    "        observation_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        observation_action_dim = observation_dim + action_dim\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_high = env.action_space.high\n",
    "        self.gamma = gamma\n",
    "        self.net_learning_rate = net_learning_rate\n",
    "        self.explore = explore\n",
    "\n",
    "        self.batches = batches\n",
    "        self.batch_size = batch_size\n",
    "        self.replayer = DQNReplayer(replayer_capacity)\n",
    "        self.replayer_initial_transitions = replayer_initial_transitions\n",
    "\n",
    "        self.noise = OrnsteinUhlenbeckProcess(size=(action_dim,),sigma=noise_scale)\n",
    "        self.noise.reset()\n",
    "\n",
    "        self.actor_evaluate_net = self.build_network(input_size=observation_dim, **actor_kwargs)\n",
    "        self.actor_target_net = self.build_network(input_size=observation_dim, **actor_kwargs)\n",
    "        self.critic_evaluate_net = self.build_network(input_size=observation_action_dim, **critic_kwargs)\n",
    "        self.critic_target_net = self.build_network(input_size=observation_action_dim, **critic_kwargs)\n",
    "\n",
    "        self.update_target_net(self.actor_target_net,\n",
    "                self.actor_evaluate_net)\n",
    "        self.update_target_net(self.critic_target_net,\n",
    "                self.critic_evaluate_net)\n",
    "\n",
    "    def update_target_net(self, target_net, evaluate_net,learning_rate=1.):\n",
    "        target_weights = target_net.get_weights()\n",
    "        evaluate_weights = evaluate_net.get_weights()\n",
    "        average_weights = [(1. - learning_rate) * t + learning_rate * e\n",
    "                    for t, e in zip(target_weights, evaluate_weights)]\n",
    "        target_net.set_weights(average_weights)\n",
    "\n",
    "    def build_network(self, input_size, hidden_sizes, output_size=1,activation=tf.nn.relu, output_activation=None,loss=tf.losses.mse, learning_rate=0.001):\n",
    "        model = keras.Sequential()\n",
    "        for layer, hidden_size in enumerate(hidden_sizes):\n",
    "            kwargs = {'input_shape' : (input_size,)} if layer == 0 else {}\n",
    "            model.add(keras.layers.Dense(units=hidden_size,activation=activation, **kwargs))\n",
    "        model.add(keras.layers.Dense(units=output_size,activation=output_activation))\n",
    "        optimizer = tf.optimizers.Adam(learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        return model\n",
    "\n",
    "    def decide(self, observation):\n",
    "        if self.explore and self.replayer.count < self.replayer_initial_transitions:\n",
    "            return np.random.uniform(self.action_low, self.action_high)\n",
    "\n",
    "        action = self.actor_evaluate_net.predict(observation[np.newaxis])[0]\n",
    "        if self.explore:\n",
    "            noise = self.noise()\n",
    "            action = np.clip(action + noise, self.action_low, self.action_high)\n",
    "        return action\n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation, done):\n",
    "        self.replayer.store(observation, action, reward, next_observation,done)\n",
    "\n",
    "        if self.replayer.count >= self.replayer_initial_transitions:\n",
    "            if done:\n",
    "                self.noise.reset() # 为下一回合重置噪声过程\n",
    "\n",
    "            for batch in range(self.batches):\n",
    "                observations, actions, rewards, next_observations,dones = self.replayer.sample(self.batch_size)\n",
    "\n",
    "                # 训练执行者网络\n",
    "                observation_tensor = tf.convert_to_tensor(observations,dtype=tf.float32)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    action_tensor = self.actor_evaluate_net(observation_tensor)\n",
    "                    input_tensor = tf.concat([observation_tensor,action_tensor], axis=1)\n",
    "                    q_tensor = self.critic_evaluate_net(input_tensor)\n",
    "                    loss_tensor = -tf.reduce_mean(q_tensor)\n",
    "                grad_tensors = tape.gradient(loss_tensor,self.actor_evaluate_net.variables)\n",
    "                self.actor_evaluate_net.optimizer.apply_gradients(zip(grad_tensors, self.actor_evaluate_net.variables))\n",
    "\n",
    "                # 训练评论者网络\n",
    "                next_actions = self.actor_target_net.predict(next_observations)\n",
    "                observation_actions = np.hstack([observations, actions])\n",
    "                next_observation_actions = np.hstack([next_observations, next_actions])\n",
    "                next_qs = self.critic_target_net.predict(next_observation_actions)[:, 0]\n",
    "                targets = rewards + self.gamma * next_qs * (1. - dones)\n",
    "                self.critic_evaluate_net.fit(observation_actions, targets,verbose=0)\n",
    "\n",
    "                self.update_target_net(self.actor_target_net,self.actor_evaluate_net, self.net_learning_rate)\n",
    "                self.update_target_net(self.critic_target_net,self.critic_evaluate_net, self.net_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_qlearning(env, agent, train=False, render=False):\n",
    "    print('play_qlearning')\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation, action, reward, next_observation, done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_observation\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_qlearning\n",
      "0  -1771.6710473932135\n",
      "play_qlearning\n",
      "1  -855.8647011844732\n",
      "play_qlearning\n",
      "2  -1547.6336582197753\n",
      "play_qlearning\n",
      "3  -879.8661867384465\n",
      "play_qlearning\n",
      "4  -1684.3831016441832\n",
      "play_qlearning\n",
      "5  -1496.9804213491143\n",
      "play_qlearning\n",
      "6  -886.604930620057\n",
      "play_qlearning\n",
      "7  -1705.0334781359381\n",
      "play_qlearning\n",
      "8  -733.8076971278624\n",
      "play_qlearning\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2b05ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2b05ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2e03e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2e03e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7f2bc98b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7f2bc98b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "9  -1665.7891678405624\n",
      "play_qlearning\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2ba8820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe7f2ba8820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "10  -1170.8628595027217\n",
      "play_qlearning\n",
      "11  -1372.1152678602239\n",
      "play_qlearning\n",
      "12  -1778.0674430903184\n",
      "play_qlearning\n",
      "13  -1698.7719431619482\n",
      "play_qlearning\n",
      "14  -1310.349081911165\n",
      "play_qlearning\n",
      "15  -1364.1406408128917\n",
      "play_qlearning\n",
      "16  -1013.1288122466768\n",
      "play_qlearning\n",
      "17  -1098.0443756159234\n",
      "play_qlearning\n",
      "18  -1173.661426882977\n",
      "play_qlearning\n",
      "19  -425.4419777875116\n",
      "play_qlearning\n",
      "20  -996.0736070562714\n",
      "play_qlearning\n",
      "21  -1122.662020638864\n",
      "play_qlearning\n",
      "22  -1085.1495868223442\n",
      "play_qlearning\n",
      "23  -998.0843525803924\n",
      "play_qlearning\n",
      "24  -794.4272089044775\n",
      "play_qlearning\n",
      "25  -931.8931819771336\n",
      "play_qlearning\n",
      "26  -843.0846063688252\n",
      "play_qlearning\n",
      "27  -531.6398326317313\n",
      "play_qlearning\n",
      "28  -399.4081377471197\n",
      "play_qlearning\n",
      "29  -529.0911427578634\n",
      "play_qlearning\n",
      "30  -576.1897417181895\n",
      "play_qlearning\n",
      "31  -275.15070583211343\n",
      "play_qlearning\n",
      "32  -9.731197820205566\n",
      "play_qlearning\n",
      "33  -275.52024238241836\n",
      "play_qlearning\n",
      "34  -1497.226206814659\n",
      "play_qlearning\n",
      "35  -267.5394697552482\n",
      "play_qlearning\n",
      "36  -398.86128130367007\n",
      "play_qlearning\n",
      "37  -6.433019597026845\n",
      "play_qlearning\n",
      "38  -139.03535147151396\n",
      "play_qlearning\n",
      "39  -411.1528800578186\n",
      "play_qlearning\n",
      "40  -1067.8088062334302\n",
      "play_qlearning\n",
      "41  -1168.8713932949818\n",
      "play_qlearning\n",
      "42  -1406.9350869626003\n",
      "play_qlearning\n",
      "43  -140.23919730956894\n",
      "play_qlearning\n",
      "44  -1259.7540458871154\n",
      "play_qlearning\n",
      "45  -126.7990780607278\n",
      "play_qlearning\n",
      "46  -118.68338360149839\n",
      "play_qlearning\n",
      "47  -532.1535582045923\n",
      "play_qlearning\n",
      "48  -11.971887950776678\n",
      "play_qlearning\n",
      "49  -145.32406988179974\n"
     ]
    }
   ],
   "source": [
    "actor_kwargs = {'hidden_sizes' : [32, 64], 'learning_rate' : 0.0001}\n",
    "critic_kwargs = {'hidden_sizes' : [64, 128], 'learning_rate' : 0.001}\n",
    "agent = DDPGAgent(env, actor_kwargs=actor_kwargs,critic_kwargs=critic_kwargs)\n",
    "\n",
    "# 训练\n",
    "episodes = 50\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_qlearning(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('{}  {}'.format(episode,episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "play_qlearning\n",
      "平均回合奖励 = -129641.6907660435 / 100 = -1296.4169076604344\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "agent.explore = False # 取消探索\n",
    "episode_rewards = [play_qlearning(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),len(episode_rewards), np.mean(episode_rewards)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
